{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "DemoGeometry.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhsilva2912/MIT_ComputerVision/blob/main/DemoGeometry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT5zZSdTjbbk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "126d3028-af98-4908-aa51-ba838b3ddd0b"
      },
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "  #copy data\n",
        "  !git clone https://github.com/mbaradad/pset_geometry\n",
        "  !cp -rf pset_geometry/* .\n",
        "  !rm -rf pset_geometry"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pset_geometry'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 46 (delta 14), reused 42 (delta 10), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (46/46), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSBX9b6Ejbbs"
      },
      "source": [
        "''' Code loading '''\n",
        "from auxiliary_functions import *\n",
        "import numpy as np\n",
        "import cv2\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "cA7aEW7Kjbb0"
      },
      "source": [
        "''' Data loading '''\n",
        "# Load the images and the ground truth depth that we will use throughout the PSET\n",
        "img_L = imread(\"data/000191_10_L.png\")\n",
        "img_R = imread(\"data/000191_10_R.png\")\n",
        "K_gt = np.load(\"data/000191_K.npy\")\n",
        "\n",
        "# Opencv expects the channels last, we define it here for compactness\n",
        "img_L_cv2 = img_L.transpose((1,2,0))\n",
        "img_R_cv2 = img_R.transpose((1,2,0))\n",
        "\n",
        "img_height = img_R.shape[1]\n",
        "img_width = img_R.shape[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIUbrGCDjbb6"
      },
      "source": [
        "In this Problem Set, we will study how to obtain the geometrical structure of a scene from either a pair of images (using photometric and geometric principles) or from a single image (by learning to predict the structure from single images with a Deep Convolutional Neural Network).\n",
        "\n",
        "The data we will use for this study is that of the KITTI dataset, and the particular example example we will use throughout this notebook is displayed in the following cell. It corresponds to one of the test images of the dataset, for which the real 3D structure is not publicly available:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p4v-Qvujbb7"
      },
      "source": [
        "show_image(img_L, title=\"Left image\")\n",
        "show_image(img_R, title=\"Right image\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ysJbG2EjbcA"
      },
      "source": [
        "To begin with, we will assume that we have access to a stereo pair of images (i.e. two images from different positions), for which we can compute how the pixels from one image relate to pixels on the other image, process which is called Stereo matching.\n",
        "\n",
        "In the following cell, the left and right images are displayed one on top of the other in a loop. As you can see, pixels that correspond to things that are further away have less disparity (\"move less\"), than pixels that correspond to things that are closer to the camera, which have higher disparity (\"move more\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM0cHX_GjbcB"
      },
      "source": [
        "display_gif('data/both.gif')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsmAvdGZjbcH"
      },
      "source": [
        "To quantify this movement between pixels, we will use the OpenCV built-in version of the algorithm \n",
        "<a href=\"https://pdfs.semanticscholar.org/bcd8/4d8bd864ff903e3fe5b91bed3f2eedacc324.pdf\"> Stereo processing by semiglobal matching and mutual information</a> by H. Hirschmuller. \n",
        "\n",
        "The following cell computes the disparity, with a set of hyperparameters that have been tuned manually to perform good in this particular instance. Once you have completed the notebook, change these parameters to see how they influence the final results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "wKnFxnK6jbcI"
      },
      "source": [
        "# Based on: http://timosam.com/python_opencv_depthimage \n",
        "window_size = 3\n",
        "min_disp = 1\n",
        "max_disp = 1 + 16*4   # max_disp - min_disp has to be dividable by 16\n",
        "left_matcher = cv2.StereoSGBM_create(\n",
        "    minDisparity=min_disp,\n",
        "    numDisparities=max_disp - min_disp,\n",
        "    blockSize=5,\n",
        "    P1=8 * 3 * window_size ** 2,\n",
        "    P2=32 * 3 * window_size ** 2,\n",
        "    disp12MaxDiff=1,\n",
        "    uniquenessRatio=15,\n",
        "    speckleWindowSize=0,\n",
        "    speckleRange=2,\n",
        "    preFilterCap=63,\n",
        "    mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\n",
        ")\n",
        "\n",
        "right_matcher = cv2.ximgproc.createRightMatcher(left_matcher)\n",
        "disparity_photometric_unfiltered = left_matcher.compute(img_L_cv2, img_R_cv2)\n",
        "show_image(disparity_photometric_unfiltered, title='Unfiltered photometric disparity', normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaAQ9RkwjbcM"
      },
      "source": [
        "To further improve the disparity prediction, it is typical to compute the disparity twice by swapping the left and right images, and enforcing that both disparities are consistent. \n",
        "The following cell achieves this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LwsXxP0jbcN"
      },
      "source": [
        "lmbda = 80000\n",
        "sigma = 1.2\n",
        "\n",
        "wls_filter = cv2.ximgproc.createDisparityWLSFilter(matcher_left=left_matcher)\n",
        "wls_filter.setLambda(lmbda)\n",
        "wls_filter.setSigmaColor(sigma)\n",
        "\n",
        "displ = disparity_photometric_unfiltered\n",
        "dispr = right_matcher.compute(img_R_cv2, img_L_cv2)\n",
        "\n",
        "displ = wls_filter.filter(displ, img_L_cv2, None, dispr)\n",
        "disparity_photometric = np.array(displ, dtype=np.float32)/16\n",
        "valid_disparity = disparity_photometric > 0\n",
        "\n",
        "show_image(disparity_photometric, title='Filtered Photometric disparity', normalize=False, show_scale=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQK-wj8FjbcU"
      },
      "source": [
        "If the cameras are <a href=\"https://en.wikipedia.org/wiki/Image_rectification\">rectified</a>, as it is the case, we can directly compute the depth (up to scale), as the inverse of the disparity as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "pe15FAS4jbcV"
      },
      "source": [
        "valid_depths = 1/disparity_photometric[valid_disparity]\n",
        "depth_photometric_unscaled = np.zeros_like(disparity_photometric)\n",
        "depth_photometric_unscaled[valid_disparity] = valid_depths\n",
        "clip_value = np.percentile(depth_photometric_unscaled.flatten(), 99)\n",
        "depth_photometric_clipped_unscaled = np.clip(depth_photometric_unscaled, 0, clip_value)\n",
        "\n",
        "show_image(depth_photometric_clipped_unscaled, title='Photometric depth unscaled', normalize=False, show_scale=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQQ_U7_hjbcZ"
      },
      "source": [
        "Finally, if we know what is the real distance between the two stereo cameras (in this case approximately 0.5335 m) and the focal length (which is stored in $K_{gt}[0][0]$ in pixel units), we can recover the real depth by considering the formula:\n",
        "\\begin{align}\n",
        "depth = \\frac{baseline \\cdot focal}{disparity}\n",
        "\\end{align}\n",
        "Which can be easily deduced using a pinhole camera model and using similar triangles: \n",
        "<img src=https://www.researchgate.net/profile/Sing_Bing_Kang/publication/2313285/figure/fig1/AS:341573188505604@1458448802555/Relationship-between-the-baseline-b-disparity-d-focal-length-f-and-depth-z.png width=\"50%\" height=\"50%\">\n",
        "\n",
        "\n",
        "Multiplying the unscaled depth by this factor we get:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqepscppjbcZ"
      },
      "source": [
        "depth_photometric = depth_photometric_unscaled*0.5335*K_gt[0,0]\n",
        "depth_photometric_clipped = np.clip(depth_photometric, 0, 80)\n",
        "show_image(depth_photometric_clipped, title='Photometric depth scaled', normalize=False, show_scale=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NUWvgbjjbcd"
      },
      "source": [
        "The second approach we will study is that of learning a model that is able to predict the disparity from a single image. To do this, we can either rely on training data (for which we have the real depth values) or, if we have access to pairs of images with translational motion, train the network without direct supervision, using similar principles as those of stereo matching.\n",
        "\n",
        "One technique that implements this type of training with a CNN is <a href=\"https://arxiv.org/abs/1806.01260\">Unsupervised Monocular Depth Estimation with Left-Right Consistency</a> by C. Godard et al., for which there is a publicly available implementation at https://github.com/mrharicot/monodepth. This method uses similar principles as <a href=\"https://arxiv.org/abs/1704.07813\">Unsupervised Learning of Depth and Ego-Motion from Video</a> T. Zhou et al., but proposes several new techniques to improve the quality of the predictions.\n",
        "\n",
        "For simplicity, we have already downloaded the models and computed the disparity predicted by this model. The following cell loads and displays the result for the test example:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "aLAvTXuwjbce"
      },
      "source": [
        "disparity_learned = np.load('precomputed_results/computed_disparity.npy')\n",
        "# Resize to the same size of the original image\n",
        "disparity_learned = cv2.resize(disparity_learned, (img_L.shape[2],img_L.shape[1]))\n",
        "depth_learned = 1/disparity_learned\n",
        "show_image(disparity_learned, title='Learned disparity', normalize=True)\n",
        "clip_value = np.percentile(depth_learned.flatten(), 95)\n",
        "depth_learned_clipped = np.clip(depth_learned, 0, clip_value)\n",
        "show_image(depth_learned_clipped, title='Learned depth', normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YYo2sv2jbci"
      },
      "source": [
        "Once we have computed disparity maps in the image plane, we can produce a point cloud, by inverting the rendering equation. \n",
        "The rendering equation relates 3D world point ($x_{w}, y_{w}, z_{w}$) to image coordinates ($x_{i}$,$y_{i}$) in the image plane following:\n",
        "\n",
        "\\begin{align}\n",
        "\\begin{bmatrix}\n",
        "       x' \\\\\n",
        "       y' \\\\\n",
        "       w\n",
        "\\end{bmatrix}&=K\n",
        "\\begin{bmatrix}\n",
        "       x_{w} \\\\\n",
        "       y_{w} \\\\\n",
        "       z_{w}\n",
        "\\end{bmatrix};\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\n",
        "x_{i} = \\frac{x'}{w}; \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\n",
        "y_{i} = \\frac{y'}{w}\n",
        "\\end{align}\n",
        "\n",
        "Consequently, to invert this process we require having access to the intrinsics matrix $K$, or approximate it with reasonable values. \n",
        "\n",
        "Assuming that the horizontal field of view of the camera is 105 degrees, and the center of projection is the center of the image, the intrinsics matrix is computed in the following cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "RMH9cnPhjbci"
      },
      "source": [
        "img_height, img_width = img_L.shape[1:3]\n",
        "def approximate_K_from_horizontal_FOV(FOV_h_deg):\n",
        "    FOV_h = np.deg2rad(FOV_h_deg) #...\n",
        "    f_x = img_width/np.tan(FOV_h/2)#...\n",
        "    f_y = img_width/np.tan(FOV_h/2) #...\n",
        "    c_x = img_width/2 #...\n",
        "    c_y = img_height/2 #...\n",
        "    K = np.array(((f_x, 0, c_x),\n",
        "                 (0, f_y, c_y),\n",
        "                 (0,   0,   1)))\n",
        "    return K\n",
        "K = approximate_K_from_horizontal_FOV(105)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J39fnLGmjbcn"
      },
      "source": [
        "Having access to $K$, we can obtain a point cloud from a depth map by simply inverting the rendering equation. First, for each pixel, we compute its coordinates in the image plane. The previous definition of $K$ assumes that the top-left pixel has the coordinate $(0,0)$, and the right most pixel the coordinate $(width,height)$. As we do not know what $w$ corresponds to each pixel, we will assume it is $1$ for the moment, and resolve the ambiguity later using the depth. The following cell  creates the $3\\times height \\times width$ coordinate map.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6zJOkFRjbco"
      },
      "source": [
        "def create_image_coord_maps(height, width):\n",
        "    img_coordinates_x_y = np.meshgrid(np.arange(0,width), np.arange(0,height))\n",
        "    img_coordinates = np.concatenate((img_coordinates_x_y, np.ones((1, height, width))))\n",
        "    return img_coordinates\n",
        "\n",
        "show_image(create_image_coord_maps(img_height, img_width)[0], normalize=True)\n",
        "show_image(create_image_coord_maps(img_height, img_width)[1], normalize=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwJPdbBIjbcr"
      },
      "source": [
        "def backproject_depth_to_pointcloud(K, depth):\n",
        "    height, width = depth.shape\n",
        "    img_coordinates = create_image_coord_maps(height, width)\n",
        "    \n",
        "    img_coordinates_flattened = img_coordinates.reshape(3,-1)\n",
        "    depth_flattened = depth.flatten()\n",
        "\n",
        "    K_inv = np.linalg.inv(K)\n",
        "    cam_coordinates = np.matmul(K_inv, img_coordinates_flattened)\n",
        "    point_cloud = cam_coordinates * depth_flattened\n",
        "\n",
        "    # Reshape to original shape\n",
        "    point_cloud = point_cloud.reshape((3, *depth.shape))\n",
        "    return point_cloud\n",
        "\n",
        "#we use clipped depths for display purposes\n",
        "point_cloud_learned = backproject_depth_to_pointcloud(K_gt, depth_learned_clipped)\n",
        "point_cloud_photometric = backproject_depth_to_pointcloud(K_gt, depth_photometric_clipped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiyQfuBJjbcv"
      },
      "source": [
        "show_pointcloud(point_cloud_learned, img_L)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_4k7XS6jbcy"
      },
      "source": [
        "show_pointcloud(point_cloud_photometric, img_L, valid_disparity, points_to_show=30000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3yLkZn-jbc1"
      },
      "source": [
        "Finally, compare the result obtained when backprojecting using the estimated K and the ground truth K, and how the result distorts depending on its values. Try it for different values of the Field of view: for example, the horizontal Field of view of an Iphone is approximately 60 degrees."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVa9_-Lzjbc1"
      },
      "source": [
        "K = approximate_K_from_horizontal_FOV(105)\n",
        "\n",
        "point_cloud_learned_estimated_K = backproject_depth_to_pointcloud(K, depth_learned_clipped)\n",
        "show_pointcloud(point_cloud_learned_estimated_K, img_L, valid_disparity, points_to_show=30000)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}