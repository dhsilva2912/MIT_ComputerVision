{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "the_problem_of_generalization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhsilva2912/MIT_ComputerVision/blob/main/the_problem_of_generalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BOMwHU3R2SF"
      },
      "source": [
        "import numpy as np              # numerical computing library\n",
        "import matplotlib.pyplot as plt # plotting library"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQvqVgsnR4BY"
      },
      "source": [
        "# create some random data\n",
        "np.random.seed(2) # set random seed for reproducibility\n",
        "\n",
        "def data_generator(Nsamples): # p_data\n",
        "  x = np.random.randn(Nsamples,1)\n",
        "  y = 0.5*x**2 + 0.2*np.random.randn(Nsamples,1)\n",
        "  return x, y\n",
        "\n",
        "# training data\n",
        "x_train, y_train = data_generator(Nsamples=10)\n",
        "\n",
        "# validation data\n",
        "x_val, y_val = data_generator(Nsamples=10)\n",
        "\n",
        "# test data\n",
        "x_test, y_test = data_generator(Nsamples=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzwvoh9xWG17"
      },
      "source": [
        "# plot the data\n",
        "def plot_data(x,y,c):\n",
        "  plt.plot(x, y, 'o', markersize=5, color=c)\n",
        "  plt.axis('equal')\n",
        "  plt.xlabel('x')\n",
        "  plt.ylabel('y')\n",
        "\n",
        "plot_data(x_train, y_train, 'blue')\n",
        "plot_data(x_val, y_val, 'red')\n",
        "# --> to try: plot test data in a different color"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DwD8kIpWKRi"
      },
      "source": [
        "# polynomial basis expansion of the data\n",
        "def basis_expansion(x,k):\n",
        "  X = np.ones(x.shape)\n",
        "  for i in range(k):\n",
        "    X = np.hstack([X, x**i]) # stack x^i in the columns of a matrix, creating the row-vector [x^0, x^1, ..., x^k]\n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ-iR-dAWh39"
      },
      "source": [
        "# least squares linear regression over the polynomial basis\n",
        "def train(x,y,k): # \"train\" and \"fit\" are interchangeable words for finding a model that minimizes prediction error on over some training data\n",
        "  X = basis_expansion(x,k)\n",
        "  w = np.linalg.lstsq(X, y, rcond=None)[0]\n",
        "  # --> question: what about the bias b? do we need it?\n",
        "  return w\n",
        "\n",
        "w = train(x_train,y_train,k=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjxfxFHJbPoV"
      },
      "source": [
        "# make a predictons for value of y over an interval of input values for x\n",
        "def predict(x,w):\n",
        "  k = w.shape[0]-1 # count up the number of basis functions that were used\n",
        "  X = basis_expansion(x,k)\n",
        "  y = np.matmul(X,w)\n",
        "  return y\n",
        "\n",
        "x_fit = np.expand_dims(np.arange(np.floor(np.min(x_train)),np.ceil(np.max(x_train)),0.1), axis=1) # interval to make predictions over\n",
        "y_fit = predict(x_fit,w)\n",
        "\n",
        "def plot_fit(x_data, y_data, x_fit, y_fit, c):\n",
        "  plot_data(x_data,y_data,c=c)\n",
        "  plt.plot(x_fit,y_fit, color=[0.3,0.3,0.3], linewidth=1)\n",
        "  plt.xlim((np.min(x_data)-1,np.max(x_data)+1))\n",
        "  plt.ylim((np.min(y_data)-1,np.max(y_data)+1))\n",
        "\n",
        "plt.figure()\n",
        "plot_fit(x_train, y_train, x_fit, y_fit, 'blue')\n",
        "plt.figure()\n",
        "plot_fit(x_val, y_val, x_fit, y_fit, 'red')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgLOpB5TLJ_-"
      },
      "source": [
        "# measure error\n",
        "def check(x,y,w):\n",
        "  y_pred = predict(x,w)\n",
        "  mse = ((y_pred-y)**2).sum()\n",
        "  return mse\n",
        "\n",
        "print('Mean squared error on train set: {:1.2f}'.format(check(x_train,y_train,w)))\n",
        "print('Mean squared error on val set: {:1.2f}'.format(check(y_val,y_val,w)))\n",
        "\n",
        "# --> to try: plot error as a function of k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxctWZ-1OVJ3"
      },
      "source": [
        "# pick the model that obtains the lowest error on the validation set\n",
        "mse_best, w_best, k_best = None, None, None\n",
        "for k in range(20):\n",
        "  w = train(x_train,y_train,k) # train\n",
        "  test_mse = check(x_test,y_test,w) # check error on the val set\n",
        "\n",
        "  if mse_best is None or test_mse < mse_best:\n",
        "    mse_best = test_mse\n",
        "    w_best = w\n",
        "    k_best = k\n",
        "\n",
        "x_fit = np.expand_dims(np.arange(np.floor(np.min(x_val)),np.ceil(np.max(x_val)),0.1), axis=1) # interval to make predictions over\n",
        "y_fit = predict(x_fit,w_best)\n",
        "plot_fit(x_val, y_val, x_fit, y_fit, 'blue')\n",
        "print('Mean squared error on val set: {:1.2f}'.format(check(x_val,y_val,w_best)))\n",
        "print('Best model using k={}'.format(k_best))\n",
        "\n",
        "# --> to try: is the best model on the val set the same as the best model on the test set? try measuring this and see. what is going on?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cI1c8AkcTUP"
      },
      "source": [
        "# regularized regression\n",
        "# --> to try: Add a regularizer that penalizes the L2 norm of the weights w. \n",
        "#             You can import sklearn.linear_model.Ridge, which will do this for you, \n",
        "#             but it is also a useful exercise to derive the closed form optimizer, \n",
        "#             which is provided in the lecture slides, and write it out in raw numpy,\n",
        "#             using matmuls, addition, etc  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}